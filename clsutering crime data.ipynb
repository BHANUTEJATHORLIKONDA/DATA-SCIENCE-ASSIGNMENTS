# -*- coding: utf-8 -*-
"""
Created on Thu Nov 23 10:58:35 2023

@author: bhanu's
"""

# import data
import pandas as pd
df = pd.read_csv("C:\\Users\\bhanu\\OneDrive\\Desktop\\data science assesments\\crime_data.csv")
df.shape
df.info()

pd.set_option('display.max_columns', None)
df

# EDA
import seaborn as sns
import matplotlib.pyplot as plt
data = ['Murder','Assault','UrbanPop','Rape']
for column in data:
    plt.figure(figsize=(8, 6))  
    sns.boxplot(x=df[column])
    plt.title("Box Plot")
    plt.show()
    

# removing the ouliers
# --> continuous variables
continuous_columns = ['Murder','Assault','UrbanPop','Rape'] 

data_without_outliers = df.copy()
for df.cloumns in continuous_columns:
    Q1 = data_without_outliers[column].quantile(0.25)
    Q3 = data_without_outliers[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_whisker_Length = Q1 - 1.5 * IQR
    upper_whisker_Length = Q3 + 1.5 * IQR
    data_without_outliers = data_without_outliers[(data_without_outliers[column] >= lower_whisker_Length) & (data_without_outliers[column]<= upper_whisker_Length)]

# data without outliers
df = data_without_outliers
df
df.shape
df.info()

# Constructing HISTOGRAM and calculating SKEWNESS AND KURTOSIS
df.hist()
df.skew()
df.kurt()
df.describe()

# understanding the relationships between all the four variables#
import matplotlib.pyplot as plt
import seaborn as sns
sns.pairplot(df, vars=['Murder', 'Assault', 'UrbanPop', 'Rape'])
plt.show()

# Generate a correlation matrix for our DataFrame
correlation_matrix = df.corr()

#Create a heatmap of the correlation matrix to gain insights
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

"""
Values close to 1 indicate a strong positive correlation.
Values close to -1 indicate a strong negative correlation.
Values close to 0 indicate a weak or no correlation
"""

# Data Transformation
X_trans = df.iloc[:,1:5]
from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
SS_X = SS.fit_transform(X_trans)
SS_X

# Construction of dendogram
import scipy.cluster.hierarchy as shc
import matplotlib.pyplot as plt
plt.figure(figsize=(10,7))
plt.title("CrimeData Dendogram")
dendo = shc.dendrogram(shc.linkage(X_trans,method = 'complete'))

# Hierarchial - Agglomerative Clustering
from sklearn.cluster import AgglomerativeClustering
cluster = AgglomerativeClustering(n_clusters = 5,affinity = 'euclidean',linkage = 'complete')
Y = cluster.fit_predict(X_trans)
Y_new = pd.DataFrame(Y)
Y_new
Y_new[0].value_counts()

# performing k means on the same data

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=5)

Y = kmeans.fit_predict(X_trans)

Y_new = pd.DataFrame(Y)
Y_new[0].value_counts()

kmeans.inertia_

kresults = []
for i in range(1,11):
    kmeans = KMeans(n_clusters=i)
    kmeans.fit_predict(X_trans)
    kresults.append(kmeans.inertia_)
    
    
kresults

import matplotlib.pyplot as plt
plt.scatter(x=range(1,11),y=kresults)
plt.plot(range(1,11),kresults,color="red")
plt.show()


#  DBSCAN
from sklearn.cluster import DBSCAN
db = DBSCAN(eps = 1.0,min_samples = 2)
X = pd.DataFrame(SS_X)
db.fit(X)

Y = db.labels_
Y_new = pd.DataFrame(Y)
Y_new[0].value_counts()

df

df_new = pd.concat([df,Y_new],axis=1)

# --> showing final data without noices
df_final = df_new[df_new[0] != -1]
df_final.shape

# --> only showing the noice data
df_noise = df_new[df_new[0] == -1]
df_noise