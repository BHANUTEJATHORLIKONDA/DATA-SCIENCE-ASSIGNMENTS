# -*- coding: utf-8 -*-
"""
Created on Thu Nov 23 11:03:15 2023

@author: bhanu's
"""

#import
import pandas as pd
df = pd.read_csv("C:\\Users\\bhanu\\OneDrive\\Desktop\\data science assesments\\wine.csv");
df
df.shape


# Exploratory Data Analysis
# Define the columns to check for outliers (excluding 'Type' (target variable) column)
columns_to_check = df.columns[1:]
columns_to_check
# Function to remove outliers using IQR
def remove_outliers_iqr(df, columns):
    for col in columns:
        Q1 = df[col].quantile(0.25)
        Q3 = df[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
    return df

# Remove outliers
data_no_outliers = remove_outliers_iqr(df, columns_to_check)
data_no_outliers  #[161 rows x 14 columns]
# Reset the index
df = data_no_outliers.reset_index(drop=True)

# Check the shape of the dataset after removing outliers
print("Shape of the dataset after removing outliers:", df.shape)  
#(161, 14)

# PCA  only for continuous variables should be done after standardization
X_trans = df.iloc[:,1:14]
X_trans

from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
SS_X = SS.fit_transform(X_trans)
SS_X
X = pd.DataFrame(SS_X)
X.columns = list(X_trans)
X

Y = df["Type"]
from sklearn.decomposition import PCA
pc = PCA()
pc1 = pc.fit_transform(X)
pc1

pc1 = pd.DataFrame(pc1)
pc1.head()

#checking variences of each column individualy for our understanding
pc1[0].var()
pc1[1].var()
pc1[2].var()
pc1[3].var()
pc1[4].var()

#calculating percentage of information for each column for our understanding
df1 = pd.DataFrame(pc.explained_variance_ratio_)
df1.sum()
df1*100

# Perform PCA to get the first three principal components
from sklearn.decomposition import PCA
pca = PCA(n_components=3)
principal_components = pca.fit_transform(X)
principal_components = pd.DataFrame(principal_components)
principal_components    
# [161 rows x 3 columns]


# performing clustering using first 3 principal component scores

#--> Hierarchical clustering
from scipy.cluster.hierarchy import dendrogram, linkage
linkage_matrix = linkage(principal_components, method='ward')
dendrogram(linkage_matrix)
import matplotlib.pyplot as plt
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Samples')
plt.ylabel('Distance')
plt.show()

#--> Agglomerative Clustering 
# Forming a group using clustering
from sklearn.cluster import AgglomerativeClustering
n_clusters = 3  # You can adjust the number of clusters as needed
agglomerative = AgglomerativeClustering(n_clusters=n_clusters,affinity='euclidean',linkage='complete')
agglomerative_labels = agglomerative.fit_predict(X)

# Visualize the clustered data
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(font_scale=1)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=agglomerative_labels, palette='viridis', s=50)
plt.title('Agglomerative Clustering')
plt.xlabel(X.columns[0])
plt.ylabel(X.columns[1])
plt.legend(title='Cluster')
plt.show()

# K-means clustering with the elbow method
from sklearn.cluster import KMeans
kresults = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=0)
    kmeans.fit(principal_components)
    kresults.append(kmeans.inertia_)

# Plot the elbow curve
import matplotlib.pyplot as plt
plt.plot(range(1, 11),kresults, marker='o')
plt.title('Elbow Curve for K-Means Clustering')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()

# Visualize the clustered data
# Visualize the first and second principal components
import seaborn as sns
import matplotlib.pyplot as plt
sns.set(font_scale=1)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=agglomerative_labels, palette='viridis', s=50)
plt.title('Agglomerative Clustering')
plt.xlabel(X.columns[0])
plt.ylabel(X.columns[1])
plt.legend(title='Cluster')
plt.show()

# Visualize the first and third principal components
sns.set(font_scale=1)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 2], hue=agglomerative_labels, palette='viridis', s=50)
plt.title('Agglomerative Clustering')
plt.xlabel(X.columns[0])
plt.ylabel(X.columns[2])
plt.legend(title='Cluster')
plt.show()

# Visualize the second and third principal components
sns.set(font_scale=1)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X.iloc[:, 1], y=X.iloc[:, 2], hue=agglomerative_labels, palette='viridis', s=50)
plt.title('Agglomerative Clustering')
plt.xlabel(X.columns[1])
plt.ylabel(X.columns[2])
plt.legend(title='Cluster')
plt.show()

# From the elbow curve, determine the optimal number of clusters
optimal_clusters = 3  # Determine based on the elbow curve

# Apply K-means clustering with the optimal number of clusters
kmeans = KMeans(n_clusters=optimal_clusters, random_state=0)
kmeans_clusters = kmeans.fit_predict(principal_components)

# Compare the obtained clusters to the original 'Type' column
original_clusters = df['Type']

# Print the results
print("Hierarchical Clustering Labels:")
print(linkage(principal_components, method='ward')[:, 2])
print("\nK-Means Clustering Labels:")
print(kmeans_clusters)
print("\nOriginal 'Type' Labels:")
print(original_clusters)