# -*- coding: utf-8 -*-
"""Copy of Text_Mining_Perform_Sentimental_Analysis_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZZ70nPNXg2X0nEmEAwET2d4fN9n9vjC

****

## **For Text Mining assignment**

*TWO:

1) Perform sentimental analysis on the Elon-musk tweets (Elon-musk.csv)*
"""

from google.colab import files
upload=files.upload()

import pandas as pd
df=pd.read_csv("Elon_musk.csv",encoding='latin1')
df

from google.colab import files
upload=files.upload()

import pandas as pd
stop=pd.read_csv("stop.txt")
stop

"""## **Sentimental Analysis**"""

# cleaning the tweets
one_tweet = df.iloc[5]['Text']
one_tweet

def TweetCleaning(tweets):
    Cleantweet=re.sub(r"@[a-zA-Z0-9]+"," ",tweets)
    Cleantweet=re.sub(r"#[a-zA-Z0-9]+"," ",Cleantweet)
    Cleantweet=''.join(word for word in Cleantweet.split() if word not in stop)
    return Cleantweet

def calPolarity(tweets):
    return TextBlob(tweets).sentiment.polarity

def calSubjectivity(tweets):
    return TextBlob(tweets).sentiment.subjectivity

def Segmentation(tweets):
  if tweets > 0:
    return "positive"
  elif tweets == 0:
    return "neutral"
  else:
    return "negative"

import re
from textblob import TextBlob

df["Cleanedtweets"]=df["Text"].apply(TweetCleaning)
df["Cleanedtweets"]

df["polarity"]=df["Cleanedtweets"].apply(calPolarity)
df["polarity"]

df["subjectivity"]=df["Cleanedtweets"].apply(calSubjectivity)
df["subjectivity"]

df["segmentation"]=df["polarity"].apply(Segmentation)
df["segmentation"]

df.head()

"""## **Analysis and visualization sentimental analysis**"""

df.pivot_table(index=['segmentation'],aggfunc={"segmentation":'count'})

# Plot the bar graph
import matplotlib.pyplot as plt
emotion_counts = df['segmentation'].value_counts()
plt.bar(emotion_counts.index, emotion_counts.values)
plt.xlabel('Emotion Label')
plt.ylabel('Count')
plt.title('Emotion Label Count')
plt.show()

# Top three positive tweets
df.sort_values(by=['polarity'],ascending=False).head(3)

# Top three nagative tweets
df.sort_values(by=['polarity'],ascending=True).head(3)

# Top three neutral tweets
df[df['polarity']==0].head(3)

"""## **Text Preprocessing**"""

book = df.iloc[:,2:3]
book

book = [x.strip() for x in book.Cleanedtweets] # remove both the leading and the trailing characters
book = [x for x in book if x] # removes empty strings, because they are considered in Python as False
book[0:10]

# Joining the list into one string/text
text = ' '.join(book)
text

!pip install nltk
import nltk
nltk.download('punkt')

#Punctuation
import string
no_punc_text = text.translate(str.maketrans('', '', string.punctuation)) #with arguments (x, y, z) where 'x' and 'y'
# must be equal-length strings and characters in 'x'
# are replaced by characters in 'y'. 'z'
# is a string (string.punctuation here)
no_punc_text

#Tokenization
from nltk.tokenize import word_tokenize
text_tokens = word_tokenize(no_punc_text)
print(text_tokens[0:50])
len(text_tokens)

#Noramalize the data
lower_words = [x.lower() for x in text_tokens]
print(lower_words[0:25])

#Stemming
from nltk.stem import PorterStemmer
ps = PorterStemmer()
stemmed_tokens = [ps.stem(word) for word in lower_words]
print(stemmed_tokens[0:40])
len(stemmed_tokens)

import spacy
# NLP english language model of spacy library
nlp = spacy.load('en_core_web_sm')
nlp

# lemmas being one of them, but mostly POS, which will follow later
doc = nlp(' '.join(stemmed_tokens))
print(doc[0:40])

lemmas = [token.lemma_ for token in doc]
print(lemmas[0:25])
len(lemmas)

# Feature Extraction
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(lemmas)

print(vectorizer.get_feature_names_out()[50:100])
print(X.toarray()[50:100])

print(X.toarray().shape)

# N- Grams
def generate_ngrams(text, n):
    words = text.split()
    ngrams = []
    for i in range(len(words) - n + 1):
        ngram = " ".join(words[i:i+n])
        ngrams.append(ngram)
    return ngrams

# Example usage
text = "I'm learning Data science as well as Data analytics. And i aslo love to learn different tools and algorithms"
n = 3  # You can change 'n' to generate different n-grams (e.g., bigrams, trigrams, etc.)

result = generate_ngrams(text, n)
print(f"{n}-grams: {result}")

def generate_ngrams_from_tokens(tokens, n):
    ngrams = []
    for i in range(len(tokens) - n + 1):
        ngram = tuple(tokens[i:i+n])
        ngrams.append(ngram)
    return ngrams

# Example usage
tokens = ["I'm", "learning", "Data science", "as", "well", "as", "Data", "analytics","And", "i", "also", "love", "to", "learn", "different", "tools", "and", "algorithms"]
n = 2  # Generating bigrams

result = generate_ngrams_from_tokens(tokens, n)
print(f"{n}-grams: {result}")

#Bi-grams
def generate_bigrams(text):
    words = text.split()
    bigrams = [(words[i], words[i + 1]) for i in range(len(words) - 1)]
    return bigrams

# Example usage
text = "I'm learning Data science as well as Data analytics. And i aslo love to learn different tools and algorithms"
print("Bi-grams:", result)

def generate_bigrams_from_tokens(tokens):
    bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]
    return bigrams

# Example usage
tokens = ["I'm", "learning", "Data science", "as", "well", "as", "Data", "analytics","And", "i", "also", "love", "to", "learn", "different", "tools", "and", "algorithms"]
result = generate_bigrams_from_tokens(tokens)
print("Bi-grams:", result)

# bigram and trigram

vectorizer_ngram_range=CountVectorizer(analyzer='word',ngram_range=(1,3),max_features=(100))
bow_matrix_ngram = vectorizer_ngram_range.fit_transform(df["Cleanedtweets"])

bow_matrix_ngram

print(vectorizer_ngram_range.get_feature_names_out())
print(bow_matrix_ngram.toarray())

# TFidf vectorizer

from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer_n_gram_max_features = TfidfVectorizer(norm="l2",analyzer='word', ngram_range=(1,3), max_features = 500)
tf_idf_matrix_n_gram_max_features =vectorizer_n_gram_max_features.fit_transform(book)
print(vectorizer_n_gram_max_features.get_feature_names_out())
print(tf_idf_matrix_n_gram_max_features.toarray())

# Commented out IPython magic to ensure Python compatibility.
# Generate wordcloud
# Import packages
import matplotlib.pyplot as plt
# %matplotlib inline
from wordcloud import WordCloud, STOPWORDS
# Define a function to plot word cloud
def plot_cloud(wordcloud):
    # Set figure size
    plt.figure(figsize=(40, 30))
    # Display image
    plt.imshow(wordcloud)
    # No axis details
    plt.axis("off");

# Generate wordcloud
stopwords = STOPWORDS
stopwords.add('will')
wordcloud = WordCloud(width = 3000, height = 2000, background_color='black', max_words=100,colormap='Set2',stopwords=stopwords).generate(text)
# Plot
plot_cloud(wordcloud)