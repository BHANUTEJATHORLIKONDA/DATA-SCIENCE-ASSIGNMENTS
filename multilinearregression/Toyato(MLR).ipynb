# -*- coding: utf-8 -*-
"""
Created on Thu Nov 23 10:34:58 2023

@author: bhanu's
"""

import pandas as pd
df = pd.read_csv("C:\\Users\\bhanu\\OneDrive\\Desktop\\data science assesments\\ToyotaCorolla.csv",encoding="latin")
df.head()

required_columns = pd.DataFrame(df)
required_columns
columns_to_drop = ['Id','Model','Mfg_Month','Mfg_Year','Fuel_Type','Met_Color','Color','Automatic','Cylinders','Mfr_Guarantee','BOVAG_Guarantee','Guarantee_Period','ABS','Airbag_1','Airbag_2','Airco','Automatic_airco','Boardcomputer','CD_Player','Central_Lock','Powered_Windows','Power_Steering','Radio','Mistlamps','Sport_Model','Backseat_Divider','Metallic_Rim','Radio_cassette','Tow_Bar']
df.drop(columns=columns_to_drop, inplace=True)

df

# EDA 
# --> BOXPLOT 
import seaborn as sns
import matplotlib.pyplot as plt
data = ['Age_08_04','KM','HP','cc','Doors','Gears','Quarterly_Tax','Weight']
for column in data:
    plt.figure(figsize=(8, 6)) 
    sns.boxplot(x=df[column])
    plt.title("Box plot")
    plt.show()

#removing the ouliers
columns = ['Age_08_04','KM','HP','cc','Doors','Gears','Quarterly_Tax','Weight']  
# Create a new DataFrame without outliers for each continuous column
data_without_outliers = df.copy()
for df.cloumns in columns:
    Q1 = data_without_outliers[column].quantile(0.25)
    Q3 = data_without_outliers[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_whisker_Length = Q1 - 1.5 * IQR
    upper_whisker_Length = Q3 + 1.5 * IQR
    data_without_outliers = data_without_outliers[(data_without_outliers[column] >= lower_whisker_Length) & (data_without_outliers[column]<= upper_whisker_Length)]
# Print the cleaned data without outliers
print(data_without_outliers)
df = data_without_outliers
print(df)
df.shape
df.info() 

# constructing histograms, skewness and kurtosis 
df["Age_08_04"].hist()
sns.distplot(df["Age_08_04"])
df["Age_08_04"].skew()
df["Age_08_04"].kurt()
df["Age_08_04"].describe()

df["KM"].hist()
sns.distplot(df["KM"])
df["KM"].skew()
df["KM"].kurt()
df["KM"].describe()

df["HP"].hist()
sns.distplot(df["HP"])
df["HP"].skew()
df["HP"].kurt()
df["HP"].describe()

df["cc"].hist()
sns.distplot(df["cc"])
df["cc"].kurt()
df["cc"].skew()
df["cc"].describe()

df["Quarterly_Tax"].hist()
sns.distplot(df["Quarterly_Tax"])
df["Quarterly_Tax"].skew()
df["Quarterly_Tax"].kurt()
df["Quarterly_Tax"].describe()

df["Doors"].hist()
sns.distplot(df["Doors"])
df["Doors"].skew()
df["Doors"].kurt()
df["Doors"].describe()

df["Gears"].hist()
sns.distplot(df["Gears"])
df["Gears"].skew()
df["Gears"].kurt()
df["Gears"].describe()

df["Weight"].hist()
sns.distplot(df["Weight"])
df["Weight"].skew()
df["Weight"].kurt()
df["Weight"].describe()

#data split and transform
# x variable
X_trans = df.iloc[:,1:9]
X_trans
list(X_trans)
from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
SS_X = SS.fit_transform(X_trans)
SS_X
X = pd.DataFrame(SS_X)
X.columns = list(X_trans)
X

# Y variable
Y_trans = df.iloc[:,0:1]
Y_trans
list(Y_trans)
from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
SS_Y = SS.fit_transform(Y_trans)
SS_Y
Y = pd.DataFrame(SS_Y)
Y.columns = list(Y_trans)
Y

#final transformed data
df_final = pd.concat([X,Y],axis = 1)
df_final

#scatter plots for all variables
import seaborn as sns
sns.set_style(style="darkgrid")
sns.pairplot(df)

#correlation values table for complete data based on these values we take the X variables one by one
pd.set_option('display.max_columns', None)	
df_final.corr()	

# in multilinear regression we check every X variable's relation with the Y variable 
# --> here we keep on adding each x variable to our model one by one so then we can descide which model is best
# --> and we need to check multi colinearity between each X variables
# x variables = Age_08_04,KM,HP,cc,Doors,Gears,Quarterly_Tax,Weight
# Y variable = Price

# Model 1
Y = df_final["Price"]
X = df_final[["Age_08_04"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.476 , R square : 77.355

# Model 2
# here we took x variables as "Age_08_04","KM" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity
Y=df_final["Age_08_04"]
X=df_final[["KM"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.336444714768849

# --> as VIF < 5 there is no multi colinearity between Age_08_04, KM so we can build model
Y = df_final["Price"]
X = df_final[["Age_08_04","KM"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.44 , R square : 80.637

# Model 3
# here we took x variables as "Age_08_04","KM","HP" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity

# checking multi colinearity between Age_08_04 , HP
Y=df_final["Age_08_04"]
X=df_final[["HP"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0106168073460788

# checking multi colinearity between KM , HP
Y=df_final["KM"]
X=df_final[["HP"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.1200004458505162


# --> as VIF < 5 there is no multi colinearity between Age_08_04, KM, HP so we can build model
Y = df_final["Price"]
X = df_final[["Age_08_04","KM","HP"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.432 , R square : 81.375

# Model 4
# here we took x variables as "Age_08_04","KM","HP","cc" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity

# checking multi colinearity between Age_08_04 , cc
Y=df_final["Age_08_04"]
X=df_final[["cc"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0000005473633573


# checking multi colinearity between KM , cc
Y=df_final["KM"]
X=df_final[["cc"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.147302046702382

# checking multi colinearity between HP , cc
Y=df_final["HP"]
X=df_final[["cc"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.001626929567329

#--> as VIF < 5 there is no multi colinearity between Age_08_04, KM, HP, cc so we can build model
Y = df_final["Price"]
X = df_final[["Age_08_04","KM","HP","cc"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.429 , R square : 81.635

# Model 5
# here we took x variables as "Age_08_04","KM","HP","cc","Doors" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity

# checking multi colinearity between Age_08_04 , Doors
Y=df_final["Age_08_04"]
X=df_final[["Doors"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0243321146000455

# checking multi colinearity between KM , Doors
Y=df_final["KM"]
X=df_final[["Doors"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0025465926246606

# checking multi colinearity between HP , Doors
Y=df_final["HP"]
X=df_final[["Doors"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0198511216020563

# checking multi colinearity between cc , Doors
Y=df_final["cc"]
X=df_final[["Doors"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0168428116668216


#--> as VIF < 5 there is no multi colinearity between Age_08_04, KM, HP, cc, Doors so we can build model
Y = df_final["Price"]
X = df_final[["Age_08_04","KM","HP","cc","Doors"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.425 , R square : 81.951

# Model 6
# here we took x variables as "Age_08_04","KM","HP","cc","Doors","Gears" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity

# checking multi colinearity between Age_08_04 , Gears
Y=df_final["Age_08_04"]
X=df_final[["Gears"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0008614709469135

# checking multi colinearity between KM , Gears
Y=df_final["KM"]
X=df_final[["Gears"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.001694982785442

# checking multi colinearity between HP , Gears
Y=df_final["HP"]
X=df_final[["Gears"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0055217124009552

# checking multi colinearity between cc , Gears
Y=df_final["cc"]
X=df_final[["Gears"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0005329169617805

# checking multi colinearity between Doors , Gears
Y=df_final["Doors"]
X=df_final[["Gears"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.024055287360224

#--> as VIF < 5 there is no multi colinearity between Age_08_04, KM, HP, cc, Doors, Gears so we can build model
Y = df_final["Price"]
X = df_final[["Age_08_04","KM","HP","cc","Doors","Gears"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.423 , R square : 82.068

# Model 7
# here we took x variables as "Age_08_04","KM","HP","cc","Doors","Gears", "Quarterly_Tax" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity

# checking multi colinearity between Age_08_04 , Quarterly_Tax
Y=df_final["Age_08_04"]
X=df_final[["Quarterly_Tax"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0007838817412686

# checking multi colinearity between KM , Quarterly_Tax
Y=df_final["KM"]
X=df_final[["Quarterly_Tax"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.177687162354058

# checking multi colinearity between HP , Quarterly_Tax
Y=df_final["HP"]
X=df_final[["Quarterly_Tax"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.2016623483867

# checking multi colinearity between cc , Quarterly_Tax
Y=df_final["cc"]
X=df_final[["Quarterly_Tax"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.5933027300984508

# checking multi colinearity between Doors , Quarterly_Tax
Y=df_final["Doors"]
X=df_final[["Quarterly_Tax"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0105380736840652

# checking multi colinearity between Gears , Quarterly_Tax
Y=df_final["Gears"]
X=df_final[["Quarterly_Tax"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.000001808634156

# --> as VIF < 5 there is no multi colinearity between Age_08_04, KM, HP, cc, Doors, Gears, Quarterly_Tax so we can build model
Y = df_final["Price"]
X = df_final[["Age_08_04","KM","HP","cc","Doors","Gears","Quarterly_Tax"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.421 , R square : 82.256

# Model 8
# here we took x variables as "Age_08_04","KM","HP","cc","Doors","Gears", "Quarterly_Tax", "Weight" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity

# checking multi colinearity between Age_08_04 , Weight
Y=df_final["Age_08_04"]
X=df_final[["Weight"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.1793165628383648

# checking multi colinearity between KM , Weight
Y=df_final["KM"]
X=df_final[["Weight"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.004554179759628

# checking multi colinearity between HP , Weight
Y=df_final["HP"]
X=df_final[["Weight"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0000805312722458

# checking multi colinearity between cc , Weight
Y=df_final["cc"]
X=df_final[["Weight"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.8431278653161849

# checking multi colinearity between Doors , Weight
Y=df_final["Doors"]
X=df_final[["Weight"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.2373203830220392

# checking multi colinearity between Gears , Weight
Y=df_final["Gears"]
X=df_final[["Weight"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0008989959358163

# checking multi colinearity between  Quarterly_Tax, Weight
Y=df_final["Quarterly_Tax"]
X=df_final[["Weight"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.3846437869675032

#--> as VIF < 5 there is no multi colinearity between Age_08_04, KM, HP, cc, Doors, Gears, Quarterly_Tax, Weight so we can build model
Y = df_final["Price"]
X = df_final[["Age_08_04","KM","HP","cc","Doors","Gears","Quarterly_Tax","Weight"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.386 , R square : 85.07

# therefore model 8 is best model among all the models as comparitively it has high R square value i.e 86.376 and low RMSE value i.e 0.369

# Residual Analysis
#fit the model with seaborn,statsmodels package
#format the plot background as scatter plots for all variables
import seaborn as sns
sns.set_style(style="darkgrid")
sns.pairplot(df_final)

#build a model
import statsmodels.formula.api as smf
model = smf.ols("Price~Age_08_04+KM+HP+cc+Doors+Gears+Quarterly_Tax+Weight",data=df_final).fit()
model.summary()


import matplotlib.pyplot as plt
import statsmodels.api as sm

qqplot = sm.qqplot(model.resid,line = "q")
plt.title("Normal Q-Q plot of residuals")
plt.show()

import numpy as np
list(np.where((model.resid) > 10))