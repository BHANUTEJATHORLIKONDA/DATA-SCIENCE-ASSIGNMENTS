# -*- coding: utf-8 -*-
"""
Created on Thu Nov 23 10:21:33 2023

@author: bhanu's
"""

#import
import pandas as pd
df = pd.read_csv("C:\\Users\\bhanu\\OneDrive\\Desktop\\data science assesments\\50_Startups.csv")
df
df.info()

#EDA
import seaborn as sns
import matplotlib.pyplot as plt
data = ['R_D_Spend','Administration','Marketing_Spend','Profit']
for column in data:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=df[column])
    plt.title("Box plot")
    plt.show()
# -->There are no outliers present 

df.hist()
df.skew()
df.kurt()

#data split and transform
X_trans = df.iloc[:,0:3]
X_trans
list(X_trans)
from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
SS_X = SS.fit_transform(X_trans)
SS_X
X1 = pd.DataFrame(SS_X)
X1.columns = list(X_trans)
X1
from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
df_sep = df[df.columns[[3]]]
df_sep
list(df_sep)
df_sep.iloc[:,0] = LE.fit_transform(df_sep.iloc[:,0])
df_sep
X2 = LE.fit_transform(df_sep.iloc[:,0])
X2
X3 = pd.DataFrame(df_sep)
X3.columns = list(df_sep)
X3
X = pd.concat([X1,X3],axis=1)
X

# for Y variable
Y_trans = df.iloc[:,4:5]
Y_trans
list(Y_trans)
from sklearn.preprocessing import StandardScaler
SS = StandardScaler()
SS_Y = SS.fit_transform(Y_trans)
SS_Y
Y = pd.DataFrame(SS_Y)
Y.columns = list(Y_trans)
Y

#final transformed data
df_final = pd.concat([X,Y],axis = 1)
df_final

#scatter plots for all variables
import seaborn as sns
sns.set_style(style="darkgrid")
sns.pairplot(df)

#correlation values table for complete data based on these values we take the X variables one by one
df_final.corr()

# in multilinear regression we check every X variable's relation with the Y variable 
# --> here we keep on adding each x variable to our model one by one so then we can descide which model is best
# --> and we need to check multi colinearity between each X variables
# x variables = R_D_Spend, Administration, Marketing Spend, State
# Y variable = Profit

# here we took x variable as "R_D_Spend" because it has high correlation among other x variables
Y = df_final["Profit"]
X = df_final[["R_D_Spend"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
#-->RMSE : 9226.101,R square : 94.654

# here we took x variables as "R_D_Spend","Administration" , so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity
Y=df_final["R_D_Spend"]
X=df_final[["Administration"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0621826590281838

# --> as VIF < 5 there is no multi colinearity between R_D_Spend, Administration so we can build model
Y = df_final["Profit"]
X = df_final[["R_D_Spend","Administration"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 9115.198, R square : 94.781

# here we took x variables as "R_D_Spend","Administration","State" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity

# checking multi colinearity between Administration , State
Y=df_final["Administration"]
X=df_final[["State"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0001403759001626

# checking multi colinearity between R_D_Spend , State
Y=df_final["R_D_Spend"]
X=df_final[["State"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0110804010836976

# --> as VIF < 5 there is no multi colinearity between R_D_Spend, Administration, State so we can build model
Y = df_final["Profit"]
X = df_final[["R_D_Spend","Administration","State"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 9115.171, R square : 94.781

# here we took x variables as "R_D_Spend","Administration","State","Marketing_Spend" so first we need to check multi colinearity between these x variables
# --> we check multicolinearity based on varience influence factor if VIF < 5 : no multi colinearity , if VIF > 5 : we have multi colinearity

# checking multi colinearity between R_D_Spend , Marketing_Spend
Y=df_final["R_D_Spend"]
X=df_final[["Marketing_Spend"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #2.103205816276043

# checking multi colinearity between Administration , Marketing_Spend
Y=df_final["Administration"]
X=df_final[["Marketing_Spend"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.0010349416824806

# checking multi colinearity between State , Marketing_Spend
Y=df_final["State"]
X=df_final[["Marketing_Spend"]] 

#fit model
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y)
LR.intercept_
LR.coef_
Y_pred = LR.predict(X)
Y_pred

#metrics
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error.round(3)))

from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",r2.round(3)*100)

VIF = 1 / (1-r2)
print("VIF :",VIF) #1.006069180313524


# --> as VIF < 5 there is no multi colinearity between R_D_Spend, Administration, State so we can build model
Y = df_final["Profit"]
X = df_final[["R_D_Spend","Administration","State","Marketing_Spend"]]
from sklearn.linear_model import LinearRegression
LR = LinearRegression()
LR.fit(X,Y) #b0+b1x1
LR.intercept_ #b0
LR.coef_ #b1
#predicted values
Y_pred = LR.predict(X)
Y_pred
#calculating sum of errors 
from sklearn.metrics import mean_squared_error
import numpy as np
error = mean_squared_error(Y, Y_pred)
print("MSE :",error.round(3))
print("RMSE :",np.sqrt(error).round(3))
#r^2 error
from sklearn.metrics import r2_score
r2 = r2_score(Y,Y_pred)
print("R square :",(r2*100).round(3))
# RMSE : 0.222, R square : 95.075

#make a table containing R^2 value for each prepared model.

M = []

# model 0
import statsmodels.formula.api as smf
model = smf.ols('Profit ~ R_D_Spend',data=df_final).fit()
m0 = model.rsquared
M.append(m0)

# model 1
import statsmodels.formula.api as smf
model = smf.ols('Profit ~ R_D_Spend+Administration',data=df_final).fit()
m1 = model.rsquared
M.append(m1)

# model 2
import statsmodels.formula.api as smf
model = smf.ols('Profit ~ R_D_Spend+Administration+State',data=df_final).fit()
m2 = model.rsquared
M.append(m2)

# model 3
import statsmodels.formula.api as smf
model = smf.ols('Profit ~ R_D_Spend+Administration+State+Marketing_Spend',data=df_final).fit()
m3 = model.rsquared
M.append(m3)

#Creating a table 
M
model = [
        [0],
        [1],
        [2],
        [3],
    ]
df = pd.DataFrame(model)
df.columns = ["Model Number"]

# Table
df1 = pd.DataFrame(M)
df1
df1.columns = ["R square values"]
df1
R_square_table = pd.concat([df,df1],axis=1)
R_square_table


# Residual Analysis
#fit the model with seaborn,statsmodels package
#format the plot background as scatter plots for all variables
import seaborn as sns
sns.set_style(style="darkgrid")
sns.pairplot(df_final)

#build a model
import statsmodels.formula.api as smf
model = smf.ols("Profit ~ R_D_Spend+Administration+State+Marketing_Spend",data=df_final).fit()
model.summary()


import matplotlib.pyplot as plt
import statsmodels.api as sm

qqplot = sm.qqplot(model.resid,line = "q")
plt.title("Normal Q-Q plot of residuals")
plt.show()

import numpy as np
list(np.where((model.resid) > 10))